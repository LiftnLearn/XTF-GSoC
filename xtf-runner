#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
xtf-runner - A utility for enumerating and running XTF tests.

Currently assumes the presence and availability of the `xl` toolstack.
"""

import sys, os, os.path as path

from optparse import OptionParser
from subprocess import Popen, PIPE, call as subproc_call, check_output

try:
    import json
except ImportError:
    import simplejson as json

# All results of a test, keep in sync with C code report.h.
# Note that warning is not a result on its own.
all_results = ('SUCCESS', 'SKIP', 'ERROR', 'FAILURE')

# Return the exit code for different states.  Avoid using 1 and 2 because
# python interpreter uses them -- see document for sys.exit.
def exit_code(state):
    """ Convert a test result to an xtf-runner exit code. """
    return { "SUCCESS": 0,
             "SKIP":    3,
             "ERROR":   4,
             "FAILURE": 5,
    }.get(state, 4)

# All test categories and configurations
all_categories   = ("special", "functional", "xsa", "utility")
pv_environments  = ("pv64", "pv32pae")
hvm_environments = ("hvm64", "hvm32pae", "hvm32pse", "hvm32")
all_environments = pv_environments + hvm_environments


class RunnerError(Exception):
    """ Errors relating to xtf-runner itself """

# Cached test json from disk
_all_test_info = {}

def get_all_test_info():
    """ Open and collate each info.json """

    # Short circuit if already cached
    if _all_test_info:
        return _all_test_info

    for test in os.listdir("tests"):

        info_file = None
        test_json = {}
        try:

            # Ignore directories which don't have a info.json inside them
            try:
                info_file = open(path.join("tests", test, "info.json"))
            except IOError:
                continue

            # Ignore tests which have bad JSON
            try:
                test_json = json.load(info_file)
            except ValueError:
                continue

            # Sanity check JSON fields and types
            if (not isinstance(test_json.get("name", None), basestring) or
                not isinstance(test_json.get("category", None), basestring) or
                not isinstance(test_json.get("environments", None), list)):
                continue

            # Sanity check JSON values
            if test_json["name"] != test:
                continue
            if test_json["category"] not in all_categories:
                continue
            for test_env in test_json["environments"]:
                if test_env not in all_environments:
                    continue

            _all_test_info[test] = test_json

        finally:
            if info_file:
                info_file.close()

    return _all_test_info


def list_tests(opts):
    """ List tests """

    args = opts.args
    cat = tuple(x for x in args if x in all_categories)
    env = tuple(x for x in args if x in all_environments)

    if opts.host:

        for line in check_output(['xl', 'info']).splitlines():
            if not line.startswith("xen_caps"):
                continue

            host_envs = []
            caps = line.split()[2:]

            if "xen-3.0-x86_64" in caps:
                host_envs.append("pv64")
            if "xen-3.0-x86_32p" in caps:
                host_envs.append("pv32pae")
            for cap in caps:
                if cap.startswith("hvm"):
                    host_envs.extend(hvm_environments)
                    break

            env = tuple(host_envs)

    all_test_info = get_all_test_info()

    for name in sorted(all_test_info.keys()):

        info = all_test_info[name]

        if cat and info["category"] not in cat:
            continue

        if env:
            for test_env in info["environments"]:
                if test_env in env:
                    break
            else:
                continue

        print name


def run_test(test):
    """ Run a specific test """

    _, _, name = test.split('-', 2)

    cfg = path.join("tests", name, test + ".cfg")

    cmd = ['xl', 'create', '-p', cfg]

    print "Executing '%s'" % (" ".join(cmd), )
    rc = subproc_call(cmd)
    if rc:
        raise RunnerError("Failed to create VM")

    cmd = ['xl', 'console', test]
    print "Executing '%s'" % (" ".join(cmd), )
    console = Popen(cmd, stdout = PIPE)

    cmd = ['xl', 'unpause', test]
    print "Executing '%s'" % (" ".join(cmd), )
    rc = subproc_call(cmd)
    if rc:
        raise RunnerError("Failed to unpause VM")

    stdout, _ = console.communicate()

    if console.returncode:
        raise RunnerError("Failed to obtain VM console")

    lines = stdout.splitlines()

    if lines:
        print "\n".join(lines)

    else:
        return "ERROR"

    test_result = lines[-1]
    if not "Test result:" in test_result:
        return "ERROR"

    for res in all_results:

        if res in test_result:
            return res

    return "ERROR"


def run_tests(opts):
    """ Run tests """

    args = opts.args
    all_test_info = get_all_test_info()
    all_test_names = all_test_info.keys()

    tests = []
    # Interpret args as a list of tests
    for arg in args:

        # If arg is a recognised test name, run every environment
        if arg in all_test_names:

            info = all_test_info[arg]

            for env in info["environments"]:
                tests.append("test-%s-%s" % (env, arg))
            continue

        # If arg is a recognised category, run every included test
        if arg in all_categories:

            for name, info in all_test_info.iteritems():

                if info["category"] == arg:

                    for env in info["environments"]:
                        tests.append("test-%s-%s" % (env, name))
            continue

        # If arg is a recognised environment, run every included test
        if arg in all_environments:

            for name, info in all_test_info.iteritems():

                if arg in info["environments"]:
                    tests.append("test-%s-%s" % (arg, name))
            continue

        parts = arg.split('-', 2)
        parts_len = len(parts)

        # If arg =~ test-$ENV-$NAME
        if parts_len == 3 and parts[0] == "test":

            # Recognised environment and test name?
            if parts[1] in all_environments and parts[2] in all_test_names:
                tests.append(arg)
                continue

            raise RunnerError("Unrecognised test '%s'" % (arg, ))

        # If arg =~ $ENV-$NAME
        if parts_len > 0 and parts[0] in all_environments:

            name = "-".join(parts[1:])

            if name in all_test_names:
                tests.append("test-" + arg)
                continue

            raise RunnerError("Unrecognised test name '%s'" % (name, ))

        # Otherwise, give up
        raise RunnerError("Unrecognised test '%s'" % (arg, ))

    if not len(tests):
        raise RunnerError("No tests to run")

    rc = all_results.index('SUCCESS')
    results = []

    for test in tests:

        res = run_test(test)
        res_idx = all_results.index(res)
        if res_idx > rc:
            rc = res_idx

        results.append(res)

    print "\nCombined test results:"

    for test, res in zip(tests, results):
        print "%-40s %s" % (test, res)

    return exit_code(all_results[rc])


def main():
    """ Main entrypoint """

    # Change stdout to be line-buffered.
    sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 1)

    # Normalise $CWD to the directory this script is in
    os.chdir(path.dirname(path.abspath(sys.argv[0])))

    # Avoid wrapping the epilog text
    OptionParser.format_epilog = lambda self, formatter: self.epilog

    parser = OptionParser(
        usage = "%prog <TEST>* | --list [<FILTER>*]",
        description = "Xen Test Framework enumeration and running tool",
        epilog = ("\n"
                  "Examples:\n"
                  "  Running tests:\n"
                  "    ./xtf-runner test-hvm32-example test-pv64-example\n"
                  "      <console ouput>\n"
                  "    Combined test results:\n"
                  "    test-hvm32-example                       SUCCESS\n"
                  "    test-pv64-example                        SUCCESS\n"
                  "    ./xtf-runner pv-iopl\n"
                  "      <console ouput>\n"
                  "    Combined test results:\n"
                  "    test-pv64-pv-iopl                        SUCCESS\n"
                  "    test-pv32pae-pv-iopl                     SUCCESS\n"
                  "\n"
                  "  Listing available tests:\n"
                  "    ./xtf-runner --list\n"
                  "       List all tests\n"
                  "    ./xtf-runner --list --host\n"
                  "       List all tests applicable for the current host\n"
                  "    ./xtf-runner --list functional special\n"
                  "       List all 'functional' or 'special' tests\n"
                  "    ./xtf-runner --list hvm64\n"
                  "       List all 'hvm64' tests\n"
                  "\n"
                  "  Exit code for this script:\n"
                  "    0:    everything is ok\n"
                  "    1,2:  reserved for python interpreter\n"
                  "    3:    test(s) are skipped\n"
                  "    4:    test(s) report error\n"
                  "    5:    test(s) report failure\n"
                  ),
    )

    parser.add_option("-l", "--list", action = "store_true",
                      dest = "list_tests",
                      help = "List available tests, optionally filtered",
                      )
    parser.add_option("--host", action = "store_true",
                      dest = "host", help = "Restrict selection to applicable"
                      " tests for the current host",
                      )

    opts, args = parser.parse_args()
    opts.args = args

    if opts.list_tests:
        return list_tests(opts)
    else:
        return run_tests(opts)


if __name__ == "__main__":
    try:
        sys.exit(main())
    except RunnerError, e:
        print >>sys.stderr, "Error:", e
        sys.exit(1)
    except KeyboardInterrupt:
        sys.exit(1)
